{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# used for testing\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordinary least square is a type of linear regression that minimizes the sum of squares between the dependent variable and the predicted value. The normal equation finds the value of parameters that minimizes the sum of squares: $b = (X^TX)^{-1}X^Ty$. In the equation, $y$ is the vector of the predicted value, $X$ is the dependent variables, and $b$ is the parameters that minizimes the sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ordinaryLeastSquare():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        X_transpose = X.T # transpose matrix\n",
    "        ols = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n",
    "        return ols\n",
    "    \n",
    "    def fit_predict(self, X, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        param = self.fit(X, y)\n",
    "        dependent = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        predicted_value = dependent.dot(param)\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Linear Regression ############\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "param = ordinaryLeastSquare().fit(X, y)\n",
    "predicted = ordinaryLeastSquare().fit_predict(X, y)\n",
    "\n",
    "param_sk = LinearRegression().fit(X, y)\n",
    "predicted_sk = param_sk.predict(X)\n",
    "\n",
    "assert np.round(param[0][0], 5) == np.round(param_sk.intercept_[0], 5)\n",
    "assert np.round(param[1][0], 5) == np.round(param_sk.coef_[0][0], 5)\n",
    "assert (predicted == predicted_sk).all\n",
    "\n",
    "############ Multiple Linear Regression ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a binary classifier and is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than 50% then the model predicts that the instance belongs to that class, and otherwise it predicts that is not. The logistic regression can be represented by the following equation:\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1 + e^{X^T\\theta}}$ where $X$ is the dependent variable and $\\theta$ is the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, fit_intercept=True):\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge Regression is a regularized version of linear regression in which a regularization term $\\alpha\\sum\\limits_{i=1}^n\\theta_i^2$ is added to the cost function. The equation for Ridge Regression is $\\theta = (X^TX + \\alpha A)^{-1}X^T y$ where $X$ is the dependent variable, $y$ is the independent variable, $\\alpha$ is the regularization strength, and $A$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Ridge Regression model\n",
    "        \n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        identity_matrix = np.identity(len(X[0]))\n",
    "        X_transpose = X.T\n",
    "        ridge_param = scipy.linalg.inv( X_transpose.dot(X) + self.alpha*(identity_matrix) ).dot(X_transpose).dot(y)\n",
    "        return ridge_param\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        param = self.fit(X, y)\n",
    "        dependent_var = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        predicted_value = dependent_var.dot(param)\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "ridge = RidgeRegression().fit(X, y)\n",
    "predict = RidgeRegression().predict(X, y)\n",
    "\n",
    "ridge_sk = Ridge(solver=\"cholesky\").fit(X, y)\n",
    "predict_sk = ridge_sk.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.43816508]\n",
      " [3.01625437]]\n",
      "\n",
      "\n",
      "[4.59142377]\n",
      "[[2.90047227]]\n"
     ]
    }
   ],
   "source": [
    "print(ridge)\n",
    "print('\\n')\n",
    "print(ridge_sk.intercept_)\n",
    "print(ridge_sk.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "no closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Gradient Descent, we need to compute the gradient of the cost function with regard to each model parameter. We use the equation $\\nabla_\\theta MSE(\\theta) = \\frac{2}{m}X^T (X\\theta - y)$ to find the gradient vector which contains all the partial derivatives of the cost function.\n",
    "\n",
    "To determine the model parameter, we use the equation $\\theta^{next step} = \\theta - \\eta\\nabla_\\theta MSE(\\theta)$ where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchGradientDescent():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, eta = 0.1, n_iterations = 1000):\n",
    "        self.eta = eta\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Batch Gradient Descent model\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones((len(X), 1)), X]\n",
    "        m = len(X)\n",
    "        theta = np.random.rand(2,1)\n",
    "        \n",
    "        for iterations in range(self.n_iterations):\n",
    "            gradient = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "            theta = theta - eta * gradient\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:2.9832341780601985, Intercept:4.513597656401026\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "bgd = BatchGradientDescent().fit(X, y)\n",
    "lr = LinearRegression().fit(X, y)\n",
    "\n",
    "assert np.round(bgd[0][0], 5) == np.round(lr.intercept_[0], 5) # check intercept\n",
    "assert np.round(bgd[1][0], 5) == np.round(lr.coef_[0][0], 5) # check coefficient\n",
    "print(\"Slope:{}, Intercept:{}\".format(bgd[1][0], bgd[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent picks a random instance in the training set at every set and computes the gradients based only on the single instance. Working on a single instance makes the algorithm run much faster.\n",
    "\n",
    "Unlike Batch Gradient Descent, the cost function will bounce up and down which decreases only on average. Over time it will end up very close to the minimum but will continue to bounce. Therefore, the final parameter values are good, but not optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, t0, t1, n_epochs=50):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        \n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # initialize values\n",
    "        m = len(X)\n",
    "        theta = np.random.randn(2, 1)\n",
    "        X = np.c_[np.ones((len(X), 1)), X]\n",
    "        \n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i in range(m):\n",
    "                random_index = np.random.randint(m)\n",
    "                xi = X[random_index:random_index+1]\n",
    "                yi = y[random_index:random_index+1]\n",
    "                \n",
    "                gradients = 2 * xi.T.dot(xi.dot(theta) - yi) # cost function\n",
    "                eta = self.learning_schedule(epoch * m + i)  # make learning schedule smaller each iteration\n",
    "                theta = theta - eta * gradients # gradient descend step\n",
    "                \n",
    "        return theta\n",
    "                \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:2.985535560235501, Intercept:4.51950801515712\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "sgd = StochasticGradientDescent(t0=5, t1=50, n_epochs=50).fit(X, y)\n",
    "print(\"Slope:{}, Intercept:{}\".format(sgd[1][0], sgd[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step, Mini-Batch Gradient Descent computes the gradients on small random sets of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, t0, t1, n_iterations = 50, minibatch_size = 20):\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.n_iterations = n_iterations\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "    def learning_schedule(self, t):\n",
    "        return self.t0 / (t + self.t1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        t = 0\n",
    "        m = len(X)\n",
    "        X = np.c_[np.ones((len(X), 1)), X]\n",
    "        theta = np.random.randn(2, 1) # random initialization\n",
    "        \n",
    "        for epoch in range(n_iterations):\n",
    "            # shuffle data\n",
    "            shuffled_indices = np.random.permutation(m)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            \n",
    "            for i in range(0, m, self.minibatch_size):\n",
    "                t += 1 # make learning schedule smaller each iteration\n",
    "                \n",
    "                # get mini-natch\n",
    "                xi = X_shuffled[i:i+self.minibatch_size]\n",
    "                yi = y_shuffled[i:i+self.minibatch_size]\n",
    "                gradients = (2/self.minibatch_size) * xi.T.dot(xi.dot(theta) - yi) # cost function\n",
    "                eta = self.learning_schedule(t)\n",
    "                theta = theta - eta * gradients # gradient descend step\n",
    "                \n",
    "        return theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope:2.983626806252817, Intercept:4.513988199056129\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "sgd = MiniBatchGradientDescent(t0=200, t1=1000).fit(X, y)\n",
    "print(\"Slope:{}, Intercept:{}\".format(sgd[1][0], sgd[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
