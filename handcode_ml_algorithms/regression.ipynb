{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# used for testing\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordinary least square is a type of linear regression that minimizes the sum of squares between the dependent variable and the predicted value. The normal equation finds the value of parameters that minimizes the sum of squares: $b = (X^TX)^{-1}X^Ty$. In the equation, $y$ is the vector of the predicted value, $X$ is the dependent variables, and $b$ is the parameters that minizimes the sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ordinaryLeastSquare():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fit_intercept=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        X_transpose = X.T # transpose matrix\n",
    "        ols = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n",
    "        return ols\n",
    "    \n",
    "    def fit_predict(self, X, y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        param = self.fit(X, y)\n",
    "        dependent = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        predicted_value = dependent.dot(param)\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Linear Regression ############\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "param = ordinaryLeastSquare().fit(X, y)\n",
    "predicted = ordinaryLeastSquare().fit_predict(X, y)\n",
    "\n",
    "param_sk = LinearRegression().fit(X, y)\n",
    "predicted_sk = param_sk.predict(X)\n",
    "\n",
    "assert np.round(param[0][0], 5) == np.round(param_sk.intercept_[0], 5)\n",
    "assert np.round(param[1][0], 5) == np.round(param_sk.coef_[0][0], 5)\n",
    "assert (predicted == predicted_sk).all\n",
    "\n",
    "############ Multiple Linear Regression ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a binary classifier and is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than 50% then the model predicts that the instance belongs to that class, and otherwise it predicts that is not. The logistic regression can be represented by the following equation:\n",
    "\n",
    "$\\sigma(t) = \\frac{1}{1 + e^{X^T\\theta}}$ where $X$ is the dependent variable and $\\theta$ is the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, fit_intercept=True):\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge Regression is a regularized version of linear regression in which a regularization term $\\alpha\\sum\\limits_{i=1}^n\\theta_i^2$ is added to the cost function. The equation for Ridge Regression is $\\theta = (X^TX + \\alpha A)^{-1}X^T y$ where $X$ is the dependent variable, $y$ is the independent variable, $\\alpha$ is the regularization strength, and $A$ is the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Ridge Regression model\n",
    "        \n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        identity_matrix = np.identity(len(X[0]))\n",
    "        X_transpose = X.T\n",
    "        ridge_param = scipy.linalg.inv( X_transpose.dot(X) + self.alpha*(identity_matrix) ).dot(X_transpose).dot(y)\n",
    "        return ridge_param\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        param = self.fit(X, y)\n",
    "        dependent_var = np.c_[np.ones((len(X), 1)), X] # add column of 1 to the beginning\n",
    "        predicted_value = dependent_var.dot(param)\n",
    "        return predicted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "ridge = RidgeRegression().fit(X, y)\n",
    "predict = RidgeRegression().predict(X, y)\n",
    "\n",
    "ridge_sk = Ridge(solver=\"cholesky\").fit(X, y)\n",
    "predict_sk = ridge_sk.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.43816508]\n",
      " [3.01625437]]\n",
      "\n",
      "\n",
      "[4.59142377]\n",
      "[[2.90047227]]\n"
     ]
    }
   ],
   "source": [
    "print(ridge)\n",
    "print('\\n')\n",
    "print(ridge_sk.intercept_)\n",
    "print(ridge_sk.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "no closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
